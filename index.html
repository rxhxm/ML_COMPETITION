<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How to Win Machine Learning Competitions: Hidden Patterns in Language
    </title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            line-height: 1.6;
            color: rgba(255, 255, 255, 0.85); /* 85% white */
            background-color: #000; /* Full black */
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            text-align: center;
            margin-bottom: 40px;
        }
        
        .avatar {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            overflow: hidden;
            margin: 0 auto 20px;
            border: 2px solid #333;
        }
        
        .avatar img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        
        h1 {
            font-size: 28px;
            margin-bottom: 10px;
        }
        
        .date {
            color: #888;
            font-size: 14px;
            margin-bottom: 30px;
        }
        
        .divider {
            border-top: 1px solid #333;
            margin: 30px 0;
        }
        
        section {
            margin-bottom: 40px;
        }
        
        h2 {
            font-size: 24px;
            margin-bottom: 20px;
            color: rgba(255, 255, 255, 0.85); /* 85% white */
        }
        
        p {
            margin-bottom: 20px;
            font-size: 16px;
        }
        
        .math {
            font-family: 'Times New Roman', Times, serif;
            font-style: italic;
            text-align: center;
            margin: 30px 0;
            font-size: 18px;
        }
        
        .graph {
            margin: 30px auto;
            text-align: center;
            max-width: 100%;
            height: 400px;
            background-color: #111;
            border-radius: 4px;
            position: relative;
            overflow: hidden;
        }
        
        .graph-title {
            text-align: center;
            color: #888;
            margin-top: 10px;
            font-size: 14px;
        }
        
        .surface {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            opacity: 0.3;
        }
        
        .point {
            position: absolute;
            width: 10px;
            height: 10px;
            background-color: #ff4500;
            border-radius: 50%;
            transform: translate(-50%, -50%);
        }
        
        .controls {
            margin-top: 20px;
            text-align: center;
        }
        
        button {
            background-color: #333;
            color: rgba(255, 255, 255, 0.85); /* 85% white */
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            margin: 0 5px;
            font-size: 14px;
        }
        
        button:hover {
            background-color: #444;
        }
    </style>
</head>
<body>
    <header>
        <div class="avatar">
            <img src="pfp.jpg" alt="Author avatar">
        </div>
        <h1>How to Win Machine Learning Competitions:<br>
            Hidden Patterns in Language
        </h1>
        <div class="date">March 22, 2025</div>
    </header>
    
    <div class="divider"></div>
    
    <section>
        <p>I told my professor I would win the competition. I didn't—I came in 2nd out of 211 students. But the near-victory taught me something far more valuable than a first-place finish: what truly separates good machine learning solutions from great ones.</p>
        
        <div style="text-align: center;">
            <iframe src="intro_3d.html" style="width: 100%; height: 400px; border: none; margin: 20px 0; display: block; background-color: #000;" title="Language Classifier Model Preview" scrolling="no"></iframe>
            <p style="font-style: italic; color: #888; margin-top: 0;">A 3D visualization of the language classifier model I developed, showing French (blue) and Spanish (gold) word clusters separated by a decision boundary. A detailed interactive version appears later in this article.</p>
        </div>
    </section>
    
    <section>
        <h1>The Challenge:</h1>
        <p>In our Machine Learning <a href="https://dsc140a.com/" style="color: rgba(255, 255, 255, 0.85); text-decoration: underline;">course</a> at UCSD, we faced a deceptively simple challenge: distinguish Spanish words from French words based solely on spelling patterns. For example, would your algorithm recognize that "meilleur" is French and "mejor" is Spanish? Both mean "best" in English, but contain linguistic patterns that distinguish their origins.</p>
        <p>What made this competition particularly interesting were the constraints:</p>
        <ol>
            <li>No third-party libraries allowed—just numpy, pandas, and scipy. No sklearn, no pre-trained models, no fancy NLP packages. This meant implementing our classifier completely from scratch. (Keep this in mind as this will become interesting later)</li>
            <li>Only 1,200 training words (roughly balanced between languages) without accents and special characters</li>
            <li>No external data sources - We couldn't download Spanish or French dictionaries to help</li>
            <li>Simple models only - We were restricted to models covered in our class, with logistic regression explicitly forbidden (nearest neighbors, linear regression, decision trees, SVMs, naive Bayes)</li>
        </ol>
        
        <p>Scoring was straightforward: 75% accuracy on the unseen test dataset earned full credit. The highest accuracy would win the competition among 211 students.</p>
        
        <p>What makes language classification fascinating is how it mirrors human intuition. Most people can guess that "meilleur" is French while "mejor" is Spanish based on subtle patterns. But how do you quantify these instincts for a computer?</p>
        
        <p>"When I first analyzed the data," I thought, "the challenge isn't building a complex model—it's helping the model 'see' what human eyes naturally detect."</p>
        
        <p>Consider this: French often uses letter combinations like "eau," "eux," and "ou," while Spanish features "ll," "rr," and "ñ." These patterns aren't random—they're linguistic fingerprints shaped by centuries of language evolution.</p>
        
        <p>This was my breakthrough realization: We often chase sophisticated algorithms when the real magic lies in how we represent the problem to the algorithm. Feature engineering—the art of transforming raw data into meaningful signals—is what separates good machine learning solutions from great ones.</p>
        
        <p>Like teaching someone to distinguish Monet from Picasso without art history knowledge, I needed to highlight the brushstrokes and color palettes that make each artist unique.</p>

        <div class="divider"></div>
        
        <h2>Try It Yourself: French or Spanish?</h2>
        <div style="margin: 30px auto; max-width: 800px; background-color: #111; border-radius: 15px; box-shadow: 0 0 30px rgba(94, 151, 246, 0.2); padding: 30px; text-align: center;">
            <p>Can you tell which language a word belongs to just by looking at it? Test your intuition:</p>
            
            <div style="font-size: 48px; margin: 20px 0; background: linear-gradient(45deg, #5E97F6, #F6BE5E); -webkit-background-clip: text; -webkit-text-fill-color: transparent; font-weight: bold;" id="wordDisplay">maison</div>
            
            <div style="display: flex; justify-content: center; gap: 20px; margin: 30px 0;">
                <button style="padding: 15px 40px; font-size: 18px; border: none; border-radius: 10px; cursor: pointer; transition: all 0.3s ease; font-weight: bold; background-color: rgba(94, 151, 246, 0.2); color: #5E97F6; border: 2px solid #5E97F6;" id="frenchBtn">French</button>
                <button style="padding: 15px 40px; font-size: 18px; border: none; border-radius: 10px; cursor: pointer; transition: all 0.3s ease; font-weight: bold; background-color: rgba(246, 190, 94, 0.2); color: #F6BE5E; border: 2px solid #F6BE5E;" id="spanishBtn">Spanish</button>
            </div>
            
            <div style="display: none; margin-top: 20px;" id="featureSelection">
                <h3>What pattern gave it away?</h3>
                <div style="display: flex; justify-content: center; gap: 15px; margin-top: 20px;">
                    <button style="background-color: #333; color: white; border: 1px solid #555; padding: 10px 15px; border-radius: 8px; cursor: pointer; transition: all 0.3s ease; flex-grow: 1; max-width: 200px;" class="feature-btn" data-feature="ending">Word Ending</button>
                    <button style="background-color: #333; color: white; border: 1px solid #555; padding: 10px 15px; border-radius: 8px; cursor: pointer; transition: all 0.3s ease; flex-grow: 1; max-width: 200px;" class="feature-btn" data-feature="letters">Letter Pattern</button>
                    <button style="background-color: #333; color: white; border: 1px solid #555; padding: 10px 15px; border-radius: 8px; cursor: pointer; transition: all 0.3s ease; flex-grow: 1; max-width: 200px;" class="feature-btn" data-feature="sound">Word Sound</button>
                    <button style="background-color: #333; color: white; border: 1px solid #555; padding: 10px 15px; border-radius: 8px; cursor: pointer; transition: all 0.3s ease; flex-grow: 1; max-width: 200px;" class="feature-btn" data-feature="length">Word Length</button>
                </div>
            </div>
            
            <div style="display: none; margin-top: 20px;" id="resultSection">
                <h3>Results</h3>
                <div style="font-size: 24px; margin: 20px 0; display: flex; justify-content: center; gap: 30px;">
                    <div style="display: flex; align-items: center; gap: 10px;">
                        Correct: <span id="correctScore" style="font-weight: bold;">0</span>
                    </div>
                    <div style="display: flex; align-items: center; gap: 10px;">
                        Incorrect: <span id="incorrectScore" style="font-weight: bold;">0</span>
                    </div>
                </div>
            </div>
        </div>
        
        <script>
            const words = [
                { word: "maison", language: "french" },
                { word: "trabajo", language: "spanish" },
                { word: "heureux", language: "french" },
                { word: "ventana", language: "spanish" },
                { word: "ville", language: "french" },
                { word: "tiempo", language: "spanish" },
                { word: "merci", language: "french" },
                { word: "gracias", language: "spanish" },
                { word: "beaucoup", language: "french" },
                { word: "hermano", language: "spanish" },
                { word: "attendre", language: "french" },
                { word: "esperar", language: "spanish" },
                { word: "maintenant", language: "french" },
                { word: "ahora", language: "spanish" },
                { word: "couleur", language: "french" },
                { word: "color", language: "spanish" }
            ];
            
            let currentWordIndex = 0;
            let correctScore = 0;
            let incorrectScore = 0;
            
            const wordDisplay = document.getElementById('wordDisplay');
            const frenchBtn = document.getElementById('frenchBtn');
            const spanishBtn = document.getElementById('spanishBtn');
            const featureSelection = document.getElementById('featureSelection');
            const resultSection = document.getElementById('resultSection');
            const correctScoreDisplay = document.getElementById('correctScore');
            const incorrectScoreDisplay = document.getElementById('incorrectScore');
            
            function setupNextRound() {
                if (currentWordIndex >= words.length) {
                    currentWordIndex = 0;
                    // Shuffle the array for next rounds
                    words.sort(() => Math.random() - 0.5);
                }
                
                const currentWord = words[currentWordIndex];
                wordDisplay.textContent = currentWord.word;
                
                frenchBtn.disabled = false;
                spanishBtn.disabled = false;
                featureSelection.style.display = 'none';
                resultSection.style.display = 'none';
                
                // Reset feature selection
                document.querySelectorAll('.feature-btn').forEach(btn => {
                    btn.classList.remove('selected');
                    btn.style.backgroundColor = '#333';
                });
            }
            
            function handleLanguageGuess(guessedLanguage) {
                const currentWord = words[currentWordIndex];
                
                frenchBtn.disabled = true;
                spanishBtn.disabled = true;
                
                if (guessedLanguage === currentWord.language) {
                    correctScore++;
                    correctScoreDisplay.textContent = correctScore;
                } else {
                    incorrectScore++;
                    incorrectScoreDisplay.textContent = incorrectScore;
                }
                
                featureSelection.style.display = 'block';
                
                document.querySelectorAll('.feature-btn').forEach(btn => {
                    btn.onclick = () => {
                        document.querySelectorAll('.feature-btn').forEach(b => {
                            b.classList.remove('selected');
                            b.style.backgroundColor = '#333';
                        });
                        btn.classList.add('selected');
                        btn.style.backgroundColor = '#5E97F6';
                        
                        setTimeout(() => {
                            resultSection.style.display = 'block';
                            currentWordIndex++;
                            
                            setTimeout(setupNextRound, 2000);
                        }, 500);
                    };
                });
            }
            
            frenchBtn.addEventListener('click', () => handleLanguageGuess('french'));
            spanishBtn.addEventListener('click', () => handleLanguageGuess('spanish'));
            
            // Start the game
            setupNextRound();
        </script>

    </section>
    
    <section>
        <h1>The Art of Feature Engineering:</h1>
        <h2>Decoding Linguistic Fingerprints</h2>
        <p>So we know humans can identify languages without knowing the meaning. So naturally, Spanish and French leave distinctive fingerprints in their words. The following patterns were derived from the dataset. Spanish loves vowel endings (154 words ending in 'o', 123 in 'a'), while French frequently ends with consonants (117 words ending in 't'). Spanish uses characteristic double letters ('rr' in "cierra," 'll' in "belleza"), while French employs unique vowel combinations ('eau' in "beaux," 'ou' in "amour").</p>
        <p>My initial approach used simple character frequencies, but accuracy plateaued at 75%. The breakthrough came when I realized word endings were disproportionately informative. The pattern 'o$' (words ending in 'o') appeared in 154 Spanish words but zero French words—making it the most predictive feature with an information gain of 0.0736.</p>
        
        <p><strong>Information gain:</strong> This metric measures how much a feature helps distinguish between classes (in this case, languages). It ranges from 0 to 1, where higher values indicate stronger predictive power. Though 0.0736 might seem modest on this scale, in language classification even values of 0.05-0.1 are considered quite significant for a single feature.</p>

        <h2>Translating Linguistics to Mathematics</h2>
        <p>To quantify these patterns, I approached the problem like a detective gathering evidence:</p>
        <ul>
            <li><strong>Extract all possible clues:</strong> I generated character n-grams (1-4 characters), position-specific patterns (prefixes and suffixes), and distinctive combinations ('eau', 'rr', etc.)
                <ul>
                    <li><strong>N-grams:</strong> Sequences of n consecutive characters. For example, in "hello", the 2-grams are "he", "el", "ll", "lo".</li>
                    <li><strong>Position-specific patterns:</strong> Characters that appear at specific positions, like the beginning (prefixes) or end (suffixes) of words.</li>
                    <li><strong>Distinctive combinations:</strong> Language-specific character sequences that might appear anywhere in a word.</li>
                </ul>
            </li>
            <li><strong>Measure each clue's value:</strong> For every pattern, I calculated information gain using this formula:
                <pre>
   def calculate_information_gain(total_french, total_spanish, pattern_french, pattern_spanish):
       # Calculate parent impurity (uncertainty before knowing the pattern)
       parent_impurity = 1 - (p_french**2 + p_spanish**2)
       
       # Calculate weighted impurity after considering the pattern
       weighted_impurity = (pattern_total / total) * pattern_impurity + 
                          (non_pattern_total / total) * non_pattern_impurity
       
       # Information gain = reduction in uncertainty
       return parent_impurity - weighted_impurity</pre>

                <div class="math">
                    IG(pattern) = H(Languages) - H(Languages | pattern)
                </div>
                <p>This code simply measures how much our uncertainty about a word's language decreases after knowing whether it contains a specific pattern.</p>
            </li>
            <li><strong>Prioritize the strongest signals:</strong> Patterns with high information gain and low Gini impurity became top features</li>
            <li><strong>Gini impurity:</strong> This measures how "mixed" a pattern is between languages. A value of 0 means the pattern appears in only one language, while higher values indicate the pattern appears in both languages. The Spanish word ending 'o$' has a perfect 0.0000 impurity—it's pure Spanish signal.</li>
        </ul>

        <p>The Spanish word ending 'o$' has a perfect 0.0000 impurity—it's pure Spanish signal.</p>

        <p>My classification approach went far beyond simple pattern detection. I engineered rich linguistic features capturing the essence of language structure – calculating vowel-to-consonant ratios, measuring character entropy (a measurement of randomness or unpredictability in a set of characters), and detecting sequence patterns like diphthongs ("ia", "ie") that are more common in Spanish. For example, calculating entropy helped identify the character diversity typical in each language:</p>
        <pre>
entropy = -sum((count / total_chars) * np.log2(count / total_chars) for count in char_counts.values())</pre>
        
        <div class="math">
            H(X) = -\sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
        </div>
        
        <p>This code calculates how unpredictable the distribution of characters is in a text sample.</p>
        
        <p>The system incorporated rule-based corrections for high-confidence patterns with definitive language markers ("ll" for Spanish, "eau" for French). This hybrid approach – statistical learning enriched with linguistic rules – addressed edge cases that pure statistical models missed.</p>
        
        <div class="divider"></div>
        
        <div id="feature-animation-container" style="margin: 30px auto; max-width: 800px; background-color: #111; border-radius: 15px; box-shadow: 0 0 30px rgba(94, 151, 246, 0.2); padding: 20px; overflow: hidden; height: 500px; display: flex; justify-content: center; align-items: center;">
            <button id="load-animation-btn" style="padding: 15px 25px; font-size: 16px; background-color: #333; color: white; border: none; border-radius: 8px; cursor: pointer;">Load Feature Animation</button>
        </div>
        
        <script>
            document.getElementById('load-animation-btn').addEventListener('click', function() {
                const container = document.getElementById('feature-animation-container');
                container.innerHTML = '<iframe src="feature_animation.html" style="width: 100%; height: 100%; border: none;" title="Feature Animation"></iframe>';
            });
        </script>

        <h2>Feature Selection: Finding Signal in Noise</h2>
        <p>My initial extraction generated 7,369 potential patterns—far too many for an efficient model and likely to overfit just to my dataset. I needed to separate signal from noise through systematic filtering:</p>
        <ul>
            <li>Minimum frequency threshold: Patterns must appear at least twice to avoid overfitting to rare words</li>
            <li>Impurity ceiling: Only patterns with Gini impurity ≤0.35 were kept</li>
            <li>Prioritize distinctive patterns: Known linguistic markers were given priority in selection</li>
        </ul>
        
        <p>The final model used 700 carefully selected features: 455 character n-grams, 64 prefix patterns, 169 suffix patterns, and 12 distinctive language-specific patterns. This approach maintained predictive power while reducing dimensionality, creating a model that was both accurate and somewhat computationally efficient.</p>

        <p>My "aha moment" came when examining top patterns by language. Spanish-indicative patterns were dominated by vowel endings ('o$', 'a$', 'do$'), while French-indicative patterns included consonant endings ('t$', 'nt$') and internal vowel combinations ('ai', 'ou').</p>

        <p>The success of this approach demonstrates the power of letting the data reveal its structure. Rather than forcing complex algorithms onto simple features, I extracted complex features that allowed even a basic ridge regression model to achieve exceptional performance.</p>

        <iframe src="features.html" style="width: 100%; height: 520px; border: none; margin: 20px 0;" title="Language Classification Features"></iframe>
    </section>
    
    <section>
        <h1>From Features to Architecture: Building a Winning Model</h1>
        <h2>Embracing Simplicity with Ridge Regression</h2>
        <p>I realized that model complexity wasn't the deciding factor. "Why use a sledgehammer when a scalpel will do?" I thought. Ridge regression became my tool of choice for three reasons:</p>
        <ul>
            <li><strong>Interpretability:</strong> With hundreds of linguistic features, I needed a model where I could trace exactly how each pattern contributed to the classification decision.</li>
            <li><strong>Efficiency:</strong> The competition's restrictions on libraries meant I needed an algorithm I could implement from scratch without excessive computational demands.</li>
            <li><strong>Robustness:</strong> Given the relatively small dataset, I needed a model resistant to overfitting and capable of generalizing to unseen words.</li>
        </ul>
        
        <p>Ridge regression's mathematical beauty lies in its modified objective function: it minimizes not just the error but also the sum of squared weights. This single regularization parameter (alpha) provided all the complexity control I needed. The closed-form solution made it computationally efficient:</p>
        <pre>
weights = np.linalg.solve(X_train.T @ X_train + alpha * np.eye(n_features), X_train.T @ y_train)</pre>

        <div class="math">
            \hat{\beta}_{ridge} = (X^TX + \alpha I)^{-1}X^Ty
        </div>
        
        <p>This code solves a system of equations to find the optimal weights while keeping them from growing too large.</p>
        
        <h2>Finding the Sweet Spot with Cross-Validation</h2>
        <p>The trickiest challenge was preventing overfitting on such a small dataset. "How do I ensure my model will generalize to the unseen test set?" This question led me to implement 5-fold cross-validation to select the optimal regularization strength.</p>
        
        <p><strong>Cross-validation:</strong> A technique where I divided the training data into 5 parts, trained on 4 parts, and tested on the remaining part – repeating this process 5 times with different test parts to get a reliable performance estimate.</p>
        
        <p>I systematically evaluated alpha values from 0.01 to 1000, measuring how each affected out-of-sample performance. This wasn't just tuning; it was learning the data's natural complexity.</p> 
        
        <p><strong>"How much should I trust these patterns versus treating them as noise?"</strong> This was the essential question my algorithm needed to answer through cross-validation.</p>

        <h2>Building an Ensemble to Embrace Uncertainty</h2>
        <p>As the competition deadline approached, I realized a single model—even well-tuned—couldn't capture all linguistic nuances. My solution? Create an ensemble of specialists.</p>
        
        <p>I built distinct models for different word types: one for short words (≤4 characters), another for longer words (>7 characters), and a specialized model for words with distinctive endings. Each model "voted" on predictions with different weights based on its expertise area.</p>
        
        <p>Bootstrap sampling added further stability—training eleven models on randomly sampled subsets of data and features. Each model captured slightly different linguistic signals, and their combined wisdom outperformed any individual predictor.</p>
        
        <h2>The Last-Minute Scramble</h2>
        <p>Here's where things got interesting. Two days before the deadline, I realized I'd been using scikit-learn's implementation of ridge regression—violating the "build from scratch" rule! This was a big "oh no" moment.</p>

        <p>I spent the final 48 hours reimplementing everything from scratch—ridge regression, cross-validation, ensemble methods—the entire architecture. The time pressure was intense, but it forced me to deeply understand every algorithm component. I submitted with a late pass, making me even more grateful for the second-place finish.</p>

        <p>The final architecture wasn't particularly fancy, but it was thoughtfully constructed at every level—from feature generation to model ensembling. It demonstrated that understanding your problem domain and applying careful feature engineering with appropriate model selection can outperform blind application of more complex algorithms.</p>

        <h2>Model Evolution and Comparison</h2>
        <p>My journey developing a Spanish/French language classifier led me to explore various architectures, with my final classify.py model achieving the highest accuracy (87.5% on train.csv) (80-20 test and validation). This model outperformed several alternatives:</p>
        
        <p>I created 7 best models (folder) and 9 worst models (folder).</p>
        
        <p>This approach significantly outperformed other models like perfect_classifier.py (81.25%), ensemble_classifier.py (81.67%), and improved_classifier.py (74.58%). While models like best_model.py and final_language_classifier.py achieved decent results (84.17%), they used off-the-shelf sklearn implementations that couldn't match my custom approach's performance and flexibility.</p>
        
        <p>The blend of machine learning with linguistic rules and specialized sub-models proved most effective for language identification, demonstrating that thoughtful feature engineering and model architecture can outperform generic solutions.</p>

        <div class="divider"></div>
        
        <h2>Model Visualization: Feature Space in 3D</h2>
        <p>To better understand how the classifier operates in a high-dimensional space, I created a 3D projection of the feature space that illustrates how Spanish and French words form distinct clusters. Below, you can explore this visualization interactively:</p>
        
        <iframe src="3d.html" style="width: 100%; height: 700px; border: none; margin: 20px 0; display: block; background-color: #000;" title="3D Language Classification Visualization" scrolling="no" allowfullscreen></iframe>
        
        <p>This visualization uses dimensionality reduction techniques to project the high-dimensional feature space into 3D, showing how the classifier separates words by language. Features are represented as vectors, with their length indicating importance. The decision boundary shows where the model transitions from classifying words as French to Spanish.</p>

        <h1>Error Analysis: Cracking the Language Code</h1>
        <p>Looking at my Spanish-French classifier's errors was like solving a linguistic detective case. "Why is my model confusing 'parte' (Spanish) with French words?" I wondered.</p>

        <p>The trickiest words fell into clear patterns: short words like "sol" (Spanish) lacked enough distinctive features, while words ending in "e" like "parte" (Spanish) and "arene" (French) confused the model because this ending is common in both languages. Cognates like "hotel" and words without strong language markers like "papel" (Spanish) or "grace" (French) were troublemakers.</p>

        <p>I discovered these patterns by tracking words misclassified across multiple cross-validation folds and looking for common characteristics. "What if these aren't random errors but systematic weaknesses?" This hunch led me to group misclassifications by word length, endings, and character patterns.</p>

        <p>This analysis sparked major improvements. I created position-sensitive features ("h" at beginning vs. middle), specialized sub-models for problematic word categories, and rule-based overrides for distinctive patterns. "If a human instantly recognizes 'ñ' as Spanish, my model should too!" These changes boosted overall test accuracy from 87.3% to 91.2%.</p>

        <p>Overfitting was unavoidable with 900+ features but only 240 training words. The solution? Strong regularization (α=10.0) and an ensemble approach that reduced overfitting by 76%, proving that sometimes the best answer isn't a single model but a carefully weighted committee.</p>
    </section>
    
    <script>
        // Simple animation to demonstrate the concept
        let animating = false;
        const point = document.getElementById('currentPoint');
        let x = 50;
        let y = 50;
        
        document.getElementById('startBtn').addEventListener('click', () => {
            if (!animating) {
                animating = true;
                animateGradientDescent();
            }
        });
        
        document.getElementById('resetBtn').addEventListener('click', () => {
            animating = false;
            x = 50;
            y = 50;
            updatePointPosition();
        });
        
        function animateGradientDescent() {
            if (!animating) return;
            
            // Simulate gradient descent by moving the point toward a minimum
            const dx = Math.random() * 2 - 1;
            const dy = Math.random() * 2 - 1;
            
            // Calculate simulated "cost" - lower near the target position (30, 70)
            const targetX = 30;
            const targetY = 70;
            const currentDistance = Math.sqrt((x - targetX) ** 2 + (y - targetY) ** 2);
            
            // Adjust position based on simulated gradient
            x -= (x - targetX) / 10 + dx;
            y -= (y - targetY) / 10 + dy;
            
            // Keep within bounds
            x = Math.max(10, Math.min(90, x));
            y = Math.max(10, Math.min(90, y));
            
            updatePointPosition();
            
            // Check if we've reached the approximate minimum
            if (currentDistance < 5) {
                setTimeout(() => {
                    animating = false;
                }, 500);
            } else {
                setTimeout(animateGradientDescent, 100);
            }
        }
        
        function updatePointPosition() {
            point.style.left = `${x}%`;
            point.style.top = `${y}%`;
        }
    </script>
</body>
</html>